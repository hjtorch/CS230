# -*- coding: utf-8 -*-
"""DNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0AFbcD0Z2lQdu_XCLGZYRhL5yHn9iv8
"""

import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

from tensorflow.python.framework import ops
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

path = "https://drive.google.com/open?id=18XCLK_REoUnPW4KjKd3ehJJppmhEqR-X"
fluff, id = path.split('=')
print (id) # Verify that you have everything after '='
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('train_data_classifier.csv')  

column_name = []
for i in range(14):
  column_name.append('SNP'+ str(i))
column_name.append('SF_Classifier')
print (column_name)

dataframe = pd.read_csv('train_data_classifier.csv', names = column_name)
print (dataframe)

dfs = np.split(dataframe, [13,14], axis=1)
#print (dfs[0])
#print (dfs[1])
X_raw = dfs[1].as_matrix()
#test_set = np.split(dataframe, [10,15,20], axis=1)
#print (test_set[0].shape)
X = X_raw
X = X.reshape(X.shape[0], -1).T
Y = dfs[1].as_matrix().T

X_train = X.T[:2400].T
X_test = X.T[2400:].T
Y_train = Y.T[:2400].T
Y_test = Y.T[2400:].T

print (X.shape)
print (X_train.shape)
print (X_test.shape)
print (Y_train.shape)
print (Y_test.shape)

def initialize_parameters(x_size, sizes):

    np.random.seed(2)                
        
    W1 = np.random.randn(sizes[0],x_size) * 0.01
    b1 = np.zeros((sizes[0], 1))

    W2 = np.random.randn(sizes[1],sizes[0]) * 0.01
    b2 = np.zeros((sizes[1], 1))

    W3 = np.random.randn(sizes[2],sizes[1]) * 0.01
    b3 = np.zeros((sizes[2], 1))

    print(W1.shape)
    print(W2.shape)
    print(W3.shape)

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2,
                  "W3": W3,
                  "b3": b3}
    
    return parameters

def sigmoid(x):
    s = 1/(1+np.exp(-x))
    return s

def forward_propagation(X, parameters):

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']

    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)
    
    assert(A3.shape == (1, X.shape[1]))
    
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2,
             "Z3": Z3,
             "A3": A3}
    
    return A3, cache

def compute_cost(A3, Y, parameters):   
    m = Y.shape[1] # number of example

    logprobs = np.multiply(np.log(A3),Y) + np.multiply((1 - Y), np.log(1 - A3))
    cost = - np.sum(logprobs) / m
    
    cost = float(np.squeeze(cost)) 
    assert(isinstance(cost, float))
    
    return cost

def backward_propagation(parameters, cache, X, Y):

    m = X.shape[1]
    
    W1 = parameters['W1']
    W2 = parameters['W2']
    W3 = parameters['W3']

    A1 = cache['A1']
    A2 = cache['A2']
    A3 = cache['A3']

    dZ3 = A3 - Y
    dW3 = 1/m * np.dot(dZ3, A2.T)
    db3 = 1/m * np.sum(dZ3, axis=1, keepdims=True)

    dZ2 = np.multiply(np.dot(W3.T, dZ3), 1 - np.power(A2, 2))
    dW2 = 1/m * np.dot(dZ2, A1.T)
    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)

    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))
    dW1 = 1/m * np.dot(dZ1, X.T)
    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2,
             "dW3": dW3,
             "db3": db3}
    
    return grads

def update_parameters(parameters, grads, learning_rate = 1.2):

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']
    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2']
    dW3 = grads['dW3']
    db3 = grads['db3']

    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    W3 = W3 - learning_rate * dW3
    b3 = b3 - learning_rate * db3
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2,
                  "W3": W3,
                  "b3": b3}
    
    return parameters

def nn_model(X, Y, num_iterations = 10000, learning_rate = 0.001, print_cost=False):

    np.random.seed(3)
    
    parameters = initialize_parameters(X.shape[0],[32,16,1])

    for i in range(0, num_iterations):
         
        A3, cache = forward_propagation(X, parameters)
        
        cost = compute_cost(A3, Y, parameters)
 
        grads = backward_propagation(parameters, cache, X, Y)
 
        parameters = update_parameters(parameters, grads, learning_rate)
        
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return parameters

def predict(parameters, X):

    A3, cache = forward_propagation(X, parameters)
    predictions = np.round(A3)
    
    return predictions

parameters = nn_model(X_train, Y_train, 10000, 0.01, print_cost=True)

print (parameters['W1'])

predictions = predict(parameters, X_test)
print ('Accuracy: %d' % float((np.dot(Y_test,predictions.T) + np.dot(1-Y_test,1-predictions.T))/float(Y_test.size)*100) + '%')

print (np.sum(predictions))
print (np.sum(Y_test))